{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb13095",
   "metadata": {},
   "source": [
    " Copyright Â© Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the\n",
    " LICENSE file in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e20dc5d",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, you will code a naive actor-critic algorithm in the tabular case. \n",
    "Then you will tune it using grid search and Bayesian optimization, \n",
    "potentially using the [optuna](https://optuna.readthedocs.io/en/stable/) library.\n",
    "Finally, you will get the best hyper-parameters obtained with both methods and perform a statistical test to see \n",
    "if there is a statistically significant difference between these methods and with respect to naive hyper-parameter values.\n",
    "\n",
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c81271",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "! pip install easypip\n",
    "from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "! pip install swig\n",
    "! pip install box2d-py\n",
    "! pip install bbrl_gymnasium\n",
    "! pip install tensorboard\n",
    "! pip install moviepy\n",
    "! pip install optuna\n",
    "! pip install mazemdp\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "if is_notebook() and get_ipython().__class__.__module__ != \"google.colab._shell\":\n",
    "   from tqdm.autonotebook import tqdm\n",
    "else:\n",
    "   from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "\n",
    "if not is_notebook():\n",
    "    print(\"Not displaying video (hidden since not in a notebook)\", file=sys.stderr)\n",
    "    def video_display(*args, **kwargs):\n",
    "        pass\n",
    "    def display(*args, **kwargs):\n",
    "        print(*args, **kwargs) \n",
    "\n",
    "testing_mode = os.environ.get(\"TESTING_MODE\", None) == \"ON\"\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "! pip install bbrl_utils\n",
    "\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=True)\n",
    "\n",
    "import hydra\n",
    "import optuna\n",
    "import yaml\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "from bbrl.utils.chrono import Chrono\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mazemdp.toolbox import sample_categorical\n",
    "from mazemdp.mdp import Mdp\n",
    "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from functools import partial\n",
    "\n",
    "# matplotlib.use(\"TkAgg\")\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "import logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b3bf3",
   "metadata": {},
   "source": [
    "# Step 1: Coding the naive Actor-critic algorithm\n",
    "\n",
    "We consider the naive actor-critic algorithm with a categorical policy.\n",
    "The algorithm learns a critic with the standard temporal difference mechanism\n",
    "using a learning rate $\\alpha_{critic}$.\n",
    "\n",
    "We consider a value-based critic $V(s)$. The extension to an action value function $Q(s,a)$ is straightforward.\n",
    "\n",
    "To update the critic, the algorithm computes the temporal difference error:\n",
    "\n",
    "$$\\delta_t = r(s_t, a_t) + \\gamma V^{(n)}(s_{t+1})-V^{(n)}(s_t).$$\n",
    "\n",
    "Then it applies it to the critic:\n",
    "\n",
    "$$V^{(n+1)}(s_t) = V^{(n)}(s_t) + \\alpha_{critic} \\delta_t.$$\n",
    "\n",
    "To update the actor, the general idea is the same, using the temporal difference error with another learning rate $\\alpha_{actor}$.\n",
    "\n",
    "However, naively applying the same learning rule would not ensure that the probabilities of all actions in a state sum to 1.\n",
    "Besides, when the temporal difference error $\\delta_t$ is negative, it may happen that the probability of an action gets negative or null, which raises an issue when applying renormalization.\n",
    "\n",
    "So, instead of applying the naive rule, we apply the following one:\n",
    "$$ \n",
    "\\pi_{temp}(a_t|s_t) =  \\begin{cases}\n",
    "\\pi^{(i)}(a_t|s_t) + \\alpha_{actor} \\delta_t & \\mathrm{if } \\pi^{(i)}(a_t|s_t) + \\alpha_{actor} \\delta_t > 10^{-8}\\\\\n",
    "10^{-8} & \\mathrm{otherwise.} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then we can apply renormalization so that the probabilities of actions still sum to 1, with\n",
    "$$\n",
    "\\forall a, \\pi^{(i+1)}(a|s_t) = \\frac{\\pi_{temp}^{(i+1)}(a|s_t)} {\\sum_{a'} \\pi_{temp}^{(i+1)}(a'|s_t)}\n",
    "$$ with\n",
    "$$ \n",
    "\\pi_{temp}^{(i+1)}(a|s_t) =  \\begin{cases}\n",
    "\\pi_{temp}(a|s_t) & \\mathrm{if } a = a_t\\\\\n",
    "\\pi^{(i)}(a|s_t) & \\mathrm{otherwise.} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Code the naive actor-critic algorithm as specified above.\n",
    "\n",
    "A good idea to build this code it to take inspiration from the code of Q-learning, to add an actor (a categorical policy), both learning rates,\n",
    "and to take care about the renormalization function.\n",
    "\n",
    "We provide some code structure below. Following this structure is not mandatory, but you should at least ensure that the signature of the ```actor_critic_v(...)``` function is respected so that the code of the next exercises can be run appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec29664",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def renormalize(\n",
    "    mdp: MazeMDPEnv,\n",
    "    policy: np.array,\n",
    "    state: np.array,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Renormalize the probability of actions so that the sum of probabilities over actions is always 1.\n",
    "    We made sure in the calling function that the probabilities of action never get negative when they are decreased.\n",
    "    :param mdp: the mdp the agent is working in (to get the number of actions)\n",
    "    :param policy: the current policy, before normalization\n",
    "    :param state: the state where the actions need to be renormalized\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "\n",
    "    # To be completed...\n",
    "\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67af9b5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_episode(\n",
    "    mdp: MazeMDPEnv,\n",
    "    policy: np.array,\n",
    "    v: np.array,\n",
    "    alpha_critic: float,\n",
    "    alpha_actor: float,\n",
    "    render: bool = True,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Perform an episode on the given mdp with the given policy\n",
    "    :param mdp: the mdp the agent is working in\n",
    "    :param policy: the current policy\n",
    "    :param v: the current critic\n",
    "    :param alpha_critic: the learning rate of the critic\n",
    "    :param alpha_actor: the learning rate of the actor\n",
    "    :return: the number of steps before it stops\n",
    "    \"\"\"\n",
    "    # Draw the first state of the episode using a uniform distribution over all the states\n",
    "    state, _ = mdp.reset(uniform=True)\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    steps = 0\n",
    "\n",
    "    # To be completed...\n",
    "\n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ab194",
   "metadata": {},
   "source": [
    "Here comes the main actor-critic function. It should take the shown parameters as input and output a list of value norms and a list of number of steps,\n",
    "corresponding to the evolution of these quantities through learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc64df",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def actor_critic_v(\n",
    "    mdp: MazeMDPEnv,\n",
    "    nb_episodes: int,\n",
    "    alpha_critic: float,\n",
    "    alpha_actor: float,\n",
    "    render: bool = True,\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    Perform actor-critic training over a number of episodes\n",
    "    :param mdp: the mdp the agent is working in (to get the number of actions to initialize the policy)\n",
    "    :param nb_episodes: the number of episodes\n",
    "    :param alpha_critic: the learning rate of the critic\n",
    "    :param alpha_actor: the learning rate of the actor\n",
    "    :param render: whether training should be rendered\n",
    "    :return: a list of norm of V values and a list of number of steps of episodes\n",
    "    \"\"\"\n",
    "\n",
    "    # To be completed...\n",
    "\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874eac2b",
   "metadata": {},
   "source": [
    "## Provide a plot function\n",
    "\n",
    "Your plot function should show the evolution through time of number of steps the agent takes to find the reward in the maze.\n",
    "If your algorithm works, this number of steps should decrease through time.\n",
    "\n",
    "Your plot function should also show a mean and a standard deviation (or some more advanced statistics) over a collection of learning runs.\n",
    "Make sure that your figure complies with [The figure checklist](https://master-dac.isir.upmc.fr/The_figure_checklist.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed...\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0186226",
   "metadata": {},
   "source": [
    "## Actor-critic hyper-parameters\n",
    "\n",
    "To represent the hyper-parameters of the experiments performed in this notebook, we suggest using the dictionary below.\n",
    "This dictionary can be read using omegaconf.\n",
    "Using it is not mandatory.\n",
    "You can also change the value of hyper-parameters or environment parameters at will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a3fc27",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "ac_params = {\n",
    "    \"save_curves\": False,\n",
    "    \"save_heatmap\": True,\n",
    "    \"mdp\": {\n",
    "        \"name\": \"MazeMDP-v0\",\n",
    "        \"width\": 5,\n",
    "        \"height\": 5,\n",
    "        \"ratio\": 0.2,\n",
    "        \"render_mode\": \"rgb_array\",\n",
    "        },\n",
    "        \n",
    "    \"log_dir\": \"./tmp\",\n",
    "    \"video_dir\": \"./tmp/videos\",\n",
    "\n",
    "    \"nb_episodes\": 100,\n",
    "    \"render\": False, # True, # \n",
    "    \"nb_repeats\": 5,\n",
    "\n",
    "    \"alpha_critic\": 0.5,\n",
    "    \"alpha_actor\": 0.5,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c825e5a",
   "metadata": {},
   "source": [
    "## Test your code\n",
    "\n",
    "The code for testing what you did so far is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mdp(cfg):\n",
    "    mdp = gym.make(cfg.mdp.name, kwargs={\"width\": cfg.mdp.width, \"height\": cfg.mdp.height, \"ratio\": cfg.mdp.ratio}, render_mode=cfg.mdp.render_mode)\n",
    "    mdp.reset()\n",
    "    return mdp\n",
    "\n",
    "cfg=OmegaConf.create(ac_params)\n",
    "mdp = make_mdp(cfg) # the mdp is made once and for all, this is an ugly global variable\n",
    "plot_steps(nb_repeats=100)\n",
    "# print(\"average v value:\", v_vals.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d71fd3",
   "metadata": {},
   "source": [
    "# Step 2: Tuning hyper-parameters\n",
    "\n",
    "In this part, you have to optimize two hyper-parameters of the actor-critic algorithm, namely the actor and critic learning rates.\n",
    "You have to do so using a simple grid search method and some Bayesian optimization method.\n",
    "For that, we suggest using [optuna](https://optuna.readthedocs.io/en/stable/).\n",
    "Follow the above link to understand how optuna works.\n",
    "The code to run optuna is provided.\n",
    "\n",
    "You should make sure that the hyper-parameters tuning algorithms that you compare benefit from the same training budget\n",
    "We suggest 400 training runs overall for each method,\n",
    "which means 20 values each for the actor and the critic learning rates in the case of grid search.\n",
    "\n",
    "By running the code below, you will do the following:\n",
    "\n",
    "1. Perform hyper-parameters tuning with two algorithms as suggested above.\n",
    "\n",
    "2. Provide a \"heatmap\" of the norm of the value function given the hyper-parameters, after training for each pair of hyper-parameters.\n",
    "\n",
    "3. Collect the value of the best hyper-parameters found with each algorithm. You will need them for Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733edc1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Sample values of alpha_critic and alpha_actor\n",
    "    alpha_critic = trial.suggest_float('alpha_critic', 0.01, 1.0)\n",
    "    alpha_actor = trial.suggest_float('alpha_actor', 0.01, 1.0)\n",
    "    \n",
    "    # Run the actor-critic algorithm with sampled hyperparameters\n",
    "    val_list, steps_list = actor_critic_v(\n",
    "        mdp,\n",
    "        alpha_critic=alpha_critic,\n",
    "        alpha_actor=alpha_actor,\n",
    "        nb_episodes=cfg.nb_episodes,  \n",
    "        render=False,  # Turn off rendering for faster runs\n",
    "    )\n",
    "\n",
    "    # We want to maximize the norm of the final value function\n",
    "    final_norm = val_list[-1]\n",
    "    return final_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72899562",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Bayesian optimization \n",
    "study_Bayes = optuna.create_study(direction='maximize')\n",
    "study_Bayes.optimize(objective, n_trials=400)\n",
    "\n",
    "# Data frame contains the params and the corresponding norm of the final value function\n",
    "study_Bayes_analyse = study_Bayes.trials_dataframe(attrs=('params', 'value')) \n",
    "\n",
    "best_Bayes = study_Bayes.best_params\n",
    "print ('The best parameters founded using Bayesian optimization are: ', best_Bayes, '\\n\\n')\n",
    "\n",
    "# Heat map\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(study_Bayes_analyse['params_alpha_critic'], \n",
    "             study_Bayes_analyse['params_alpha_actor'], \n",
    "             c=study_Bayes_analyse['value'], cmap=\"RdYlGn_r\")\n",
    "plt.title('Value function norms (Bayesian optimization)')\n",
    "plt.xlabel(r'$\\alpha_{critic}$')\n",
    "plt.ylabel(r'$\\alpha_{actor}$')\n",
    "plt.colorbar()\n",
    "plt.savefig(\"bayes_opt.pdf\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "search_space = {'alpha_critic': np.linspace(0.01, 1.0, 20), 'alpha_actor': np.linspace(0.01, 1.0, 20)}\n",
    "study_grid = optuna.create_study(direction='maximize', sampler=optuna.samplers.GridSampler(search_space))\n",
    "study_grid.optimize(objective)\n",
    "\n",
    "# Data frame contains the params and the corresponding norm of the final value function\n",
    "study_grid_analyse = study_grid.trials_dataframe(attrs=('params', 'value')) \n",
    "\n",
    "best_grid = study_grid.best_params\n",
    "print ('The best parameters founded using Grid search are: ', best_grid, '\\n\\n')\n",
    "\n",
    "# Heat map\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(study_grid_analyse['params_alpha_critic'], \n",
    "             study_grid_analyse['params_alpha_actor'], \n",
    "             c=study_grid_analyse['value'], cmap=\"RdYlGn_r\")\n",
    "plt.title('Value function norms (Grid search)')\n",
    "plt.xlabel(r'$\\alpha_{critic}$')\n",
    "plt.ylabel(r'$\\alpha_{actor}$')\n",
    "plt.grid(False)\n",
    "plt.colorbar()\n",
    "plt.savefig(\"grid_search.pdf\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59359ec7",
   "metadata": {},
   "source": [
    "# Step 3: Statistical tests\n",
    "\n",
    "Now you have to compare the performance of the actor-critic algorithm tuned\n",
    "with all the best hyper-parameters you found before, using statistical tests.\n",
    "\n",
    "The functions below are provided to run Welch's T-test over learning curves.\n",
    "They have been adapted from a github repository: https://github.com/flowersteam/rl_stats\n",
    "You don't need to understand them in detail (though it is always a good idea to try to understand more code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6530ef",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import bootstrapped.bootstrap as bs\n",
    "import bootstrapped.compare_functions as bs_compare\n",
    "import bootstrapped.stats_functions as bs_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d17eff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_central_tendency_and_error(id_central, id_error, sample):\n",
    "\n",
    "    try:\n",
    "        id_error = int(id_error)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if id_central == \"mean\":\n",
    "        central = np.nanmean(sample, axis=1)\n",
    "    elif id_central == \"median\":\n",
    "        central = np.nanmedian(sample, axis=1)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if isinstance(id_error, int):\n",
    "        low = np.nanpercentile(sample, q=int((100 - id_error) / 2), axis=1)\n",
    "        high = np.nanpercentile(sample, q=int(100 - (100 - id_error) / 2), axis=1)\n",
    "    elif id_error == \"std\":\n",
    "        low = central - np.nanstd(sample, axis=1)\n",
    "        high = central + np.nanstd(sample, axis=1)\n",
    "    elif id_error == \"sem\":\n",
    "        low = central - np.nanstd(sample, axis=1) / np.sqrt(sample.shape[0])\n",
    "        high = central + np.nanstd(sample, axis=1) / np.sqrt(sample.shape[0])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return central, low, high\n",
    "\n",
    "def run_test(data1, data2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compute tests comparing data1 and data2 with confidence level alpha\n",
    "    :param data1: (np.ndarray) sample 1\n",
    "    :param data2: (np.ndarray) sample 2\n",
    "    :param alpha: (float) confidence level of the test\n",
    "    :return: (bool) if True, the null hypothesis is rejected\n",
    "    \"\"\"\n",
    "    data1 = data1.squeeze()\n",
    "    data2 = data2.squeeze()\n",
    "\n",
    "    # perform Welch t-test\":\n",
    "    _, p = ttest_ind(data1, data2, equal_var=False)\n",
    "    return p < alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c1e33",
   "metadata": {},
   "source": [
    "This last function was adapted for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_test(perf1, perf2, name1, name2, sample_size=20, downsampling_fact=5, confidence_level=0.01):\n",
    "\n",
    "    perf1 = perf1.transpose()\n",
    "    perf2 = perf2.transpose()\n",
    "    nb_datapoints = perf1.shape[1]\n",
    "    nb_steps = perf1.shape[0]\n",
    "\n",
    "    legend = [name1, name2]\n",
    "\n",
    "    # what do you want to plot ?\n",
    "    id_central = 'mean' # \"median\"  # \n",
    "    id_error = 80  # (percentiles), also: 'std', 'sem'\n",
    "\n",
    "    sample1 = perf1[:, np.random.randint(0, nb_datapoints, sample_size)]\n",
    "    sample2 = perf2[:, np.random.randint(0, nb_datapoints, sample_size)]\n",
    "\n",
    "    steps = np.arange(0, nb_steps, downsampling_fact)\n",
    "    sample1 = sample1[steps, :]\n",
    "    sample2 = sample2[steps, :]\n",
    "\n",
    "    # test\n",
    "    sign_diff = np.zeros([len(steps)])\n",
    "    for i in range(len(steps)):\n",
    "        sign_diff[i] = run_test(\n",
    "            sample1[i, :], sample2[i, :], alpha=confidence_level\n",
    "        )\n",
    "\n",
    "    central1, low1, high1 = compute_central_tendency_and_error(\n",
    "        id_central, id_error, sample1\n",
    "    )\n",
    "    central2, low2, high2 = compute_central_tendency_and_error(\n",
    "        id_central, id_error, sample2\n",
    "    )\n",
    "\n",
    "    # plot\n",
    "    _, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    lab1 = plt.xlabel(\"training steps\")\n",
    "    lab2 = plt.ylabel(\"performance\")\n",
    "\n",
    "    plt.plot(steps*downsampling_fact, central1, linewidth=10)\n",
    "    plt.plot(steps*downsampling_fact, central2, linewidth=10)\n",
    "    plt.fill_between(steps*downsampling_fact, low1, high1, alpha=0.3)\n",
    "    plt.fill_between(steps*downsampling_fact, low2, high2, alpha=0.3)\n",
    "    leg = ax.legend(legend, frameon=False)\n",
    "\n",
    "    # plot significative difference as dots\n",
    "    idx = np.argwhere(sign_diff == 1)\n",
    "    y = max(np.nanmax(high1), np.nanmax(high2))\n",
    "    # Plot the stars where there is a statistically significant difference\n",
    "    plt.scatter(steps[idx]*downsampling_fact, y * 1.05 * np.ones([idx.size]), s=100, c=\"k\", marker=\"*\")\n",
    "\n",
    "    # style\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(10.0)\n",
    "    ax.spines[\"top\"].set_linewidth(5)\n",
    "    ax.spines[\"right\"].set_linewidth(5)\n",
    "    ax.spines[\"bottom\"].set_linewidth(5)\n",
    "    ax.spines[\"left\"].set_linewidth(5)\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"./{name1}_{name2}.png\", bbox_extra_artists=(leg, lab1, lab2), bbox_inches=\"tight\", dpi=100\n",
    "    )\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1773d",
   "metadata": {},
   "source": [
    "It is now time to compare the performance of you actor-critic algorithm, using different hyper-parameters.\n",
    "As hyper-parameters, you will use:\n",
    "\n",
    "- naive tuning, that is a pair (0.5, 0.5) for the actor and critic learning rates,\n",
    "- the best hyper-parameters you found with the different tuning algorithms you used before.\n",
    "\n",
    "Below, we provide the code for comparing the Bayesian optimization approach and grid search.\n",
    "You have to extend it to also compare these two approaches to using naive tuning.\n",
    "\n",
    "The code below does the following:\n",
    "\n",
    "1. For each set of hyper-parameters, collect a large dataset of learning curves (we suggest using 150 training episodes)\n",
    "\n",
    "2. Perform statistical comparisons\n",
    "\n",
    "- Take two datasets of learning curves obtained with the hyper-parameters sets that you found with different tuning algorithms.\n",
    "- Use the ``` perform_test(...)``` function to compare each possible pair of sets.\n",
    "\n",
    "You should obtain an image for each pair you have tried.\n",
    "In this image, black dots signal the time step where there is a statistically significant difference between two learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_runs = 150\n",
    "# Collect dataset\n",
    "perf_naive = [] # Dataset for naive tuning \n",
    "perf_Bayes = [] # Dataset for Bayesian optimization\n",
    "perf_grid = [] # Dataset for Grid search\n",
    "\n",
    "for _ in range(nb_runs):\n",
    "    v_Bayes, _ = actor_critic_v(mdp, nb_episodes=cfg.nb_episodes,\n",
    "                                   alpha_critic=best_Bayes['alpha_critic'], alpha_actor=best_Bayes['alpha_actor'],\n",
    "                                   render=False)\n",
    "    v_grid, _ = actor_critic_v(mdp, nb_episodes=cfg.nb_episodes,\n",
    "                                   alpha_critic=best_grid['alpha_critic'], alpha_actor=best_grid['alpha_actor'],\n",
    "                                   render=False)\n",
    "\n",
    "    perf_Bayes.append(v_Bayes)\n",
    "    perf_grid.append(v_grid)\n",
    "\n",
    "perf_Bayes = np.array(perf_Bayes)\n",
    "perf_grid = np.array(perf_grid)\n",
    "    \n",
    "# Perform statistical comparisons \n",
    "perform_test(perf_Bayes, perf_grid, 'Bayes optimization', 'Grid search')\n",
    "    \n",
    "    \n",
    "# To be completed...\n",
    "\n",
    "assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "# Once you have completed the code, conclude"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
