{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f794410c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    " Copyright Â© Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f2b4eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, we study the **value iteration** and **policy\n",
    "iteration** algorithms in a maze environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d9ebe",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cad17a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[easypip] Installing bbrl_utils\n",
      "ERROR:root:pip install returned an error for: /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/bin/python -m pip install bbrl_utils\n",
      "ERROR:root:Collecting bbrl_utils\n",
      "  Using cached bbrl_utils-0.10.3-py3-none-any.whl.metadata (443 bytes)\n",
      "Requirement already satisfied: easypip in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl_utils) (1.3.5)\n",
      "Requirement already satisfied: bbrl>=0.3.1 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl_utils) (0.3.3)\n",
      "Collecting bbrl-gymnasium>=0.3.5 (from bbrl_utils)\n",
      "  Using cached bbrl_gymnasium-0.3.7-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tensorboard in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl_utils) (2.18.0)\n",
      "Collecting pygame (from bbrl_utils)\n",
      "  Using cached pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: torch>=1.9.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (2.5.1)\n",
      "Requirement already satisfied: torchvision in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (0.20.1)\n",
      "Requirement already satisfied: tqdm in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (4.67.1)\n",
      "Requirement already satisfied: hydra-core in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (1.3.2)\n",
      "Requirement already satisfied: numpy in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (2.2.2)\n",
      "Requirement already satisfied: pandas in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (2.2.3)\n",
      "Requirement already satisfied: opencv-python in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (4.11.0.86)\n",
      "Requirement already satisfied: omegaconf in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (2.3.0)\n",
      "Requirement already satisfied: matplotlib in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (3.10.0)\n",
      "Requirement already satisfied: seaborn in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (0.13.2)\n",
      "Requirement already satisfied: scipy in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (1.15.1)\n",
      "Requirement already satisfied: bootstrapped in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (0.0.2)\n",
      "Requirement already satisfied: gymnasium in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (1.0.0)\n",
      "Requirement already satisfied: moviepy in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from bbrl>=0.3.1->bbrl_utils) (2.1.2)\n",
      "Collecting gymnasium (from bbrl>=0.3.1->bbrl_utils)\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mazemdp>=1.2.0 (from bbrl-gymnasium>=0.3.5->bbrl_utils)\n",
      "  Using cached mazemdp-1.2.7-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: packaging in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from easypip->bbrl_utils) (24.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from tensorboard->bbrl_utils) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from tensorboard->bbrl_utils) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from tensorboard->bbrl_utils) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from tensorboard->bbrl_utils) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from tensorboard->bbrl_utils) (75.8.0)\n",
      "Requirement already satisfied: six>1.9 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from tensorboard->bbrl_utils) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from tensorboard->bbrl_utils) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from tensorboard->bbrl_utils) (3.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from gymnasium->bbrl>=0.3.1->bbrl_utils) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from gymnasium->bbrl>=0.3.1->bbrl_utils) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from gymnasium->bbrl>=0.3.1->bbrl_utils) (0.0.4)\n",
      "Collecting ipyreact (from mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils)\n",
      "  Using cached ipyreact-0.5.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (3.17.0)\n",
      "Requirement already satisfied: networkx in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.9.0->bbrl>=0.3.1->bbrl_utils) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->bbrl_utils) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from matplotlib->bbrl>=0.3.1->bbrl_utils) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from matplotlib->bbrl>=0.3.1->bbrl_utils) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from matplotlib->bbrl>=0.3.1->bbrl_utils) (4.55.7)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from matplotlib->bbrl>=0.3.1->bbrl_utils) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from matplotlib->bbrl>=0.3.1->bbrl_utils) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from matplotlib->bbrl>=0.3.1->bbrl_utils) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from matplotlib->bbrl>=0.3.1->bbrl_utils) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from pandas->bbrl>=0.3.1->bbrl_utils) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from pandas->bbrl>=0.3.1->bbrl_utils) (2025.1)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d]->bbrl-gymnasium>=0.3.5->bbrl_utils)\n",
      "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting swig==4.* (from gymnasium[box2d]->bbrl-gymnasium>=0.3.5->bbrl_utils)\n",
      "  Using cached swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from hydra-core->bbrl>=0.3.1->bbrl_utils) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from omegaconf->bbrl>=0.3.1->bbrl_utils) (6.0.2)\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from moviepy->bbrl>=0.3.1->bbrl_utils) (5.1.1)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from moviepy->bbrl>=0.3.1->bbrl_utils) (2.37.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from moviepy->bbrl>=0.3.1->bbrl_utils) (0.6.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from moviepy->bbrl>=0.3.1->bbrl_utils) (0.1.10)\n",
      "Requirement already satisfied: python-dotenv>=0.10 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from moviepy->bbrl>=0.3.1->bbrl_utils) (1.0.1)\n",
      "Collecting anywidget>=0.2.0 (from ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils)\n",
      "  Using cached anywidget-0.9.13-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting ipywidgets>=7.0.0 (from ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils)\n",
      "  Using cached ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting psygnal>=0.8.1 (from anywidget>=0.2.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils)\n",
      "  Using cached psygnal-0.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (8.31.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils)\n",
      "  Using cached widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils)\n",
      "  Using cached jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: exceptiongroup in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.0.0->ipyreact->mazemdp>=1.2.0->bbrl-gymnasium>=0.3.5->bbrl_utils) (0.2.3)\n",
      "Using cached bbrl_utils-0.10.3-py3-none-any.whl (11 kB)\n",
      "Using cached bbrl_gymnasium-0.3.7-py3-none-any.whl (26 kB)\n",
      "Using cached pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "Using cached mazemdp-1.2.7-py3-none-any.whl (18 kB)\n",
      "Using cached swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
      "Using cached ipyreact-0.5.0-py3-none-any.whl (1.8 MB)\n",
      "Using cached anywidget-0.9.13-py3-none-any.whl (213 kB)\n",
      "Using cached ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Using cached psygnal-0.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (727 kB)\n",
      "Using cached widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py): started\n",
      "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for box2d-py\n",
      "Failed to build box2d-py\n",
      "\n",
      "ERROR:root:  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31mÃ\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31mâ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31mâ°â>\u001b[0m \u001b[31m[16 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Using setuptools (version 75.8.0).\n",
      "  \u001b[31m   \u001b[0m /home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages/setuptools/_distutils/dist.py:270: UserWarning: Unknown distribution option: 'test_suite'\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg)\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/Box2D\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/__init__.py -> build/lib.linux-x86_64-cpython-310/Box2D\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/Box2D.py -> build/lib.linux-x86_64-cpython-310/Box2D\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/Box2D/b2\n",
      "  \u001b[31m   \u001b[0m copying library/Box2D/b2/__init__.py -> build/lib.linux-x86_64-cpython-310/Box2D/b2\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'Box2D._Box2D' extension\n",
      "  \u001b[31m   \u001b[0m swigging Box2D/Box2D.i to Box2D/Box2D_wrap.cpp\n",
      "  \u001b[31m   \u001b[0m swig -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library/Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D/Box2D_wrap.cpp Box2D/Box2D.i\n",
      "  \u001b[31m   \u001b[0m error: command 'swig' failed: No such file or directory\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/bin/python', '-m', 'pip', 'install', 'bbrl_utils']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m testing_mode \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTESTING_MODE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mON\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[43measyimport\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbbrl_utils\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msetup(maze_mdp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbbrl_gymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaze_mdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MazeMDPEnv\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmazemdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mdp\n",
      "File \u001b[0;32m~/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages/easypip/__init__.py:150\u001b[0m, in \u001b[0;36measyimport\u001b[0;34m(spec, ask)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reqs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly one package should be mentioned in the specification\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m (req,) \u001b[38;5;241m=\u001b[39m reqs\n\u001b[0;32m--> 150\u001b[0m \u001b[43m_install\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(req\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m~/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages/easypip/__init__.py:134\u001b[0m, in \u001b[0;36m_install\u001b[0;34m(req, ask)\u001b[0m\n\u001b[1;32m    129\u001b[0m         answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule is not installed. Install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreq\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m? [y/n] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         )\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ask \u001b[38;5;129;01mor\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mInstaller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot installing as required\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/lib/python3.10/site-packages/easypip/__init__.py:115\u001b[0m, in \u001b[0;36mInstaller.install\u001b[0;34m(requirement, extra_args)\u001b[0m\n\u001b[1;32m    107\u001b[0m command \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    108\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexecutable,\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-m\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mstr\u001b[39m(requirement),\n\u001b[1;32m    113\u001b[0m ] \u001b[38;5;241m+\u001b[39m extra_args\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    117\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip install returned an error for: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(command))\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:526\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 526\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    527\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/home/ari-d/Documents/Google Drive Temporary/Polytech/S8/PPO/TPs/ROB4_S8_POO/bin/python', '-m', 'pip', 'install', 'bbrl_utils']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError:\n",
    "    !pip install easypip\n",
    "    !pip install swig\n",
    "    # Automatically restart the kernel after installation\n",
    "    import IPython\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) \n",
    "\n",
    "from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "# easyinstall(\"bbrl_gymnasium>=0.3.5\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from moviepy.editor import ipython_display as video_display\n",
    "\n",
    "if not is_notebook():\n",
    "    print(\"Not displaying video (hidden since not in a notebook)\", file=sys.stderr)\n",
    "    def video_display(*args, **kwargs):\n",
    "        pass\n",
    "    def display(*args, **kwargs):\n",
    "        print(*args, **kwargs) \n",
    "\n",
    "testing_mode = os.environ.get(\"TESTING_MODE\", None) == \"ON\"\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "! pip install box2d-py\n",
    "! pip install bbrl_utils\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=True)\n",
    "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
    "from mazemdp.mdp import Mdp\n",
    "from mazemdp.toolbox import egreedy, egreedy_loc, sample_categorical, softmax\n",
    "from mazemdp import random_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dad39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install easypip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4355d1",
   "metadata": {},
   "source": [
    "# Agents and MDPs\n",
    "\n",
    "A reinforcement learning agent interacts with an environment represented as a\n",
    "Markov Decision Process (MDP). It is defined by a tuple $(S, A, P, r, \\gamma)$\n",
    "where $S$ is the state space, $A$ is the action space, $P(state_t, action_t,\n",
    "state_{t+1})$ is the transition function, $r(state_t, action_t)$ is the reward\n",
    "function and $\\gamma \\in [0, 1]$ is the discount factor.\n",
    "\n",
    "In what follows we import code to create an MDP corresponding to a random maze\n",
    "(see https://github.com/osigaud/SimpleMazeMDP for documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"MazeMDP-v0\",\n",
    "    kwargs={\"width\": 5, \"height\": 5, \"ratio\": 0.2},\n",
    "    render_mode=\"human\",\n",
    ")\n",
    "env = env.unwrapped\n",
    "env.reset()\n",
    "\n",
    "# in dynamic programming, there is no agent moving in the environment\n",
    "env.set_no_agent()\n",
    "env.init_draw(\"The maze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ccda8",
   "metadata": {},
   "source": [
    "Dynamic programming\n",
    "\n",
    "The goal of an RL agent is to find the optimal behaviour, defined by a policy\n",
    "$\\pi$ that assigns an action (or distribution over actions) to each state so\n",
    "as to maximize the agent's total expected reward. In order to estimate how\n",
    "good a state is, either a state value function $V(x)$ or a state-action value\n",
    "function $Q(x,u)$ is used.\n",
    "\n",
    "Dynamic programming algorithms are used for planning, they require a full\n",
    "knowledge of the MDP from the agent (in contrast to \"true\" RL where the agent\n",
    "does not know the transition and reward functions). They find the optimal\n",
    "policy by computing a value function $V$ or an action-value function $Q$ over\n",
    "the state space or state-action space of the given MDP. **Value iteration**\n",
    "and **policy iteration** are two standard dynamic programming algorithms. You\n",
    "should study both of them using both $V$ and $Q$, as these algorithms contain\n",
    "the basic building blocks for most RL algorithms.\n",
    "\n",
    "# Value Iteration\n",
    "\n",
    "## Value Iteration with the V function\n",
    "\n",
    "When using the $V$ function, **value iteration** aims at finding the optimal\n",
    "values $V^*$ based on the Bellman Optimality Equation: $$V^*(s) = \\max_a\n",
    "\\big[r(s,a) + \\gamma \\sum_{y \\in S} P(s,a,y)V^*(y) \\big],$$ where:\n",
    "\n",
    "*   $r(s, a)$ is the reward obtained from taking action $a$ in state $s$,\n",
    "*   $P(s, a, y)$ is the probability of reaching state $y$ when taking action\n",
    "    $a$ in state $s$,\n",
    "*   $\\gamma \\in [0,1]$ is a discount factor defining the relative importance\n",
    "    of long term rewards over short term ones (the closer to 0, the more the\n",
    "    agent focuses on immediate rewards).\n",
    "\n",
    "In practice, we start with an initial value function $V^0$ (for instance, the\n",
    "values of all states are 0), and then we iterate for all states $s$\n",
    "$$V^{i+1}(s) = \\max_a \\big[ r(s,a) + \\gamma \\sum_{y \\in S} P(s,a,y)V^i(y)\n",
    "\\big],$$\n",
    "\n",
    "until the values converge, that is $\\forall s, V^{i+1}(s) \\approx V^i(s)$. It\n",
    "is shown that at convergence, $\\forall s, V^i(s)= V^*(s)$.\n",
    "\n",
    "To visualize the policy obtained from **value iteration**, we need to first\n",
    "define the `get_policy_from_V()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912057d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_policy_from_v(mdp: MazeMDPEnv, v: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Outputs a policy given the state values\"\"\"\n",
    "\n",
    "    # Sets initial state values are set to 0\n",
    "    policy = np.zeros(mdp.nb_states)  \n",
    "\n",
    "    # Loop over MDP states\n",
    "    for x in range(mdp.nb_states):\n",
    "        if x in mdp.terminal_states:\n",
    "            # Takes the reward associated with the terminal state\n",
    "            policy[x] = np.argmax(mdp.r[x, :])\n",
    "        else:\n",
    "            # Compute the value V(x) for state x\n",
    "            v_temp = []\n",
    "            \n",
    "            # Loop over actions\n",
    "            for u in range(mdp.action_space.n):\n",
    "                # Process sum of the values of the neighbouring states\n",
    "                summ = 0\n",
    "                for y in range(mdp.nb_states):\n",
    "                    summ = summ + mdp.P[x, u, y] * v[y]\n",
    "                v_temp.append(mdp.r[x, u] + mdp.gamma * summ)\n",
    "            policy[x] = np.argmax(v_temp)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495ef76",
   "metadata": {},
   "source": [
    "The `value_iteration_v(mdp)` function below provides the code of **value\n",
    "iteration** using the $V$ function. It is given as an example from which you\n",
    "can derive other instances of dynamic programming algorithms. Look at it more\n",
    "closely, this will help for later questions:\n",
    "\n",
    "* you can ignore the `mdp.new_render()` and `mdp.render(...)` functions which\n",
    "  are here to provide the visualization of the iterations.\n",
    "* find in the code the loop over states, the main loop that performs these\n",
    "  updates until the values don't change significantly anymore, the main update\n",
    "  equation. Found them? OK, you can continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294822ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_v(\n",
    "    mdp: MazeMDPEnv, render: bool = True\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    # Value Iteration using the state value v\n",
    "    v = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    v_list = []\n",
    "    stop = False\n",
    "\n",
    "    mdp.init_draw(\"Value iteration V\")\n",
    "\n",
    "    mdp.draw_v(v)\n",
    "\n",
    "    while not stop:\n",
    "        v_old = v.copy()\n",
    "        mdp.draw_v(v)\n",
    "\n",
    "        for x in range(mdp.nb_states):  # for each state x\n",
    "            # Compute the value of the state x for each action u of the MDP action space\n",
    "            if x in mdp.terminal_states:\n",
    "                v[x] = np.max(mdp.r[x, :])\n",
    "            else:\n",
    "                v_temp = []\n",
    "                for u in range(mdp.action_space.n):\n",
    "                    # Process sum of the values of the neighbouring states\n",
    "                    summ = 0\n",
    "                    for y in range(mdp.nb_states):\n",
    "                        summ = summ + mdp.P[x, u, y] * v_old[y]\n",
    "                    v_temp.append(mdp.r[x, u] + mdp.gamma * summ)\n",
    "\n",
    "                # Select the highest state value among those computed\n",
    "                v[x] = np.max(v_temp)\n",
    "\n",
    "        # Test if convergence has been reached\n",
    "        if (np.linalg.norm(v - v_old)) < 0.01:\n",
    "            stop = True\n",
    "        v_list.append(np.linalg.norm(v))\n",
    "\n",
    "    policy = get_policy_from_v(mdp, v)\n",
    "    mdp.draw_v_pi(v, policy)\n",
    "    return v, v_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ff447",
   "metadata": {},
   "source": [
    "Let us run it on the previously defined MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91d537",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "v, v_list = value_iteration_v(env, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d50b096",
   "metadata": {},
   "source": [
    "### Value iteration with the $Q$ function ###\n",
    "\n",
    "The state-action value function $Q^{\\pi}(s,a)$ defines the value of being in\n",
    "state $s$, taking action $a$ then following policy $\\pi$. The Bellman\n",
    "Optimality Equation for $Q^*$ is $$ Q^*(s,a) =  r(s,a) + \\gamma \\sum_{y}\n",
    "P(s,a,y) \\max_{a'}Q^*(y,a'). $$\n",
    "\n",
    "**Question:** By taking inspiration from the `value_iteration_v(mdp)` function\n",
    "above, fill the blank (given with '\\#Q[x,u]=...') in the code of\n",
    "`value_iteration_q(mdp)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Value Iteration with the Q function ---------------------#\n",
    "# Given a MDP, this algorithm computes the optimal action value function Q\n",
    "# It then derives the optimal policy based on this function\n",
    "\n",
    "\n",
    "def value_iteration_q(\n",
    "    mdp: MazeMDPEnv, render: bool = True\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    q = np.zeros(\n",
    "        (mdp.nb_states, mdp.action_space.n)\n",
    "    )  # initial action values are set to 0\n",
    "    q_list = []\n",
    "    stop = False\n",
    "\n",
    "    mdp.init_draw(\"Value iteration Q\")\n",
    "\n",
    "    mdp.draw_v(q)\n",
    "\n",
    "    while not stop:\n",
    "        qold = q.copy()\n",
    "\n",
    "        mdp.draw_v(q)\n",
    "\n",
    "        for x in range(mdp.nb_states):\n",
    "            for u in range(mdp.action_space.n):\n",
    "                if x in mdp.terminal_states:\n",
    "                    q[x, u] = mdp.r[x, u]\n",
    "                else:\n",
    "                    summ = 0\n",
    "                    for y in range(mdp.nb_states):\n",
    "                        summ += mdp.P[x, u, y] * np.max(qold[y, :])\n",
    "\n",
    "                    # ComplÃ©ter d'aprÃ¨s la formule ci-dessus\n",
    "\n",
    "                    q[x, u] = ...\n",
    "                    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        if (np.linalg.norm(q - qold)) <= 0.01:\n",
    "            stop = True\n",
    "        q_list.append(np.linalg.norm(q))\n",
    "\n",
    "    mdp.draw_v(q)\n",
    "\n",
    "    return q, q_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91896bde",
   "metadata": {},
   "source": [
    "Once you are done, run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce41dc5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "q, q_list = value_iteration_q(env, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac3a4a4",
   "metadata": {},
   "source": [
    "## Policy Iteration ##\n",
    "\n",
    "The **policy iteration** algorithm is more complicated than **value\n",
    "iteration**. Given a MDP and a policy $\\pi$, **policy iteration** iterates the\n",
    "following steps:\n",
    "\n",
    "*   Evaluate policy $\\pi$: compute $V$ or $Q$ based on the policy $\\pi$;\n",
    "*   Improve policy $\\pi$: compute a better policy based on $V$ or $Q$.\n",
    "\n",
    "This process is repeated until convergence, i.e. when the policy cannot be\n",
    "improved anymore.\n",
    "\n",
    "### Policy iteration with the $V$ function ###\n",
    "\n",
    "When using $V$, $V^{\\pi}(s)$ is the expected return when starting from state\n",
    "$s$ and following policy $\\pi$. It is processed based on the Bellman\n",
    "Optimality Equation for deterministic policies:\n",
    "\n",
    "$$V^\\pi(s) = r(s, \\pi(s)) + \\gamma \\sum_{y \\in S}P(s, \\pi(s), y)V^\\pi(y),$$\n",
    "\n",
    "where:\n",
    "\n",
    "*   $\\pi$ is a deterministic policy, meaning that in a state $s$, the agent\n",
    "    always selects the same action,\n",
    "*   $V^\\pi(y)$ is the value of the state $y$ under policy $\\pi$.\n",
    "\n",
    "Thus, given a policy $\\pi$, one must first compute its value function\n",
    "$V^\\pi(s)$ for all states $s$ iterating the Bellman Optimality Equation until\n",
    "convergence, that is using **value iteration**. Then, one must determine if\n",
    "policy $\\pi$ can be improved based on $V$. For that, in each state $s$, one\n",
    "can compute the Q-value $Q(s,a)$ of applying action $a$ and then following\n",
    "policy $\\pi$ based on the just computed $V^\\pi$, and replace the action\n",
    "$\\pi(s)$ with $\\arg\\max_a Q(s,a)$.\n",
    "\n",
    "In order to facilitate the coding of **policy iteration** algorithms, we first\n",
    "define a set of useful functions.\n",
    "\n",
    "The `improve_policy_from_v(mdp, v, policy)` function is very similar to the\n",
    "`get_policy_from_v(v)` function which was given above. The main difference is\n",
    "that it takes a policy as argument and improves this policy when possible,\n",
    "thus is more in the spirit of the `policy improvement` step of **policy\n",
    "iteration**. But both functions can be used interchangeably.\n",
    "\n",
    "The functions `evaluate_one_step_v(mdp, v, policy)`, where `mdp` is a given\n",
    "MDP, `v` is some value function in this MDP and `policy` is some policy and\n",
    "the function `evaluate_v(mdp, policy)` are also given. These functions are\n",
    "used to build the value function $V^\\pi$ corresponding to policy $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f19b1b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def improve_policy_from_v(\n",
    "    mdp: MazeMDPEnv, v: np.ndarray, policy: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    # Improves a policy given the state values\n",
    "    for x in range(mdp.nb_states):  # for each state x\n",
    "        # Compute the value of the state x for each action u of the MDP action space\n",
    "        if x in mdp.terminal_states:\n",
    "            policy[x] = np.argmax(mdp.r[x, :])\n",
    "        else:\n",
    "            v_temp = np.zeros(mdp.action_space.n)\n",
    "            for u in range(mdp.action_space.n):\n",
    "                # Process sum of the values of the neighbouring states\n",
    "                summ = 0\n",
    "                for y in range(mdp.nb_states):\n",
    "                    summ = summ + mdp.P[x, u, y] * v[y]\n",
    "                v_temp[u] = mdp.r[x, u] + mdp.gamma * summ\n",
    "\n",
    "            for u in range(mdp.action_space.n):\n",
    "                if v_temp[u] > v_temp[policy[x]]:\n",
    "                    policy[x] = u\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40082d72",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def evaluate_one_step_v(\n",
    "    mdp: MazeMDPEnv, v: np.ndarray, policy: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    # Outputs the state value function after one step of policy evaluation\n",
    "    # Corresponds to one application of the Bellman Operator\n",
    "    v_new = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    for x in range(mdp.nb_states):  # for each state x\n",
    "        # Compute the value of the state x for each action u of the MDP action space\n",
    "        if x in mdp.terminal_states:\n",
    "            v_new[x] = mdp.r[x, policy[x]]\n",
    "        else:\n",
    "            # Process sum of the values of the neighbouring states\n",
    "            summ = 0\n",
    "            for y in range(mdp.nb_states):\n",
    "                summ = summ + mdp.P[x, policy[x], y] * v[y]\n",
    "            v_new[x] = mdp.r[x, policy[x]] + mdp.gamma * summ\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc532bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def evaluate_v(mdp: MazeMDPEnv, policy: np.ndarray) -> np.ndarray:\n",
    "    # Outputs the state value function of a policy\n",
    "    v = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        vold = v.copy()\n",
    "        v = evaluate_one_step_v(mdp, vold, policy)\n",
    "\n",
    "        # Test if convergence has been reached\n",
    "        if (np.linalg.norm(v - vold)) < 0.01:\n",
    "            stop = True\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbfd5a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "To perform **policy iteration** we also need an initial random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mazemdp import random_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe994084",
   "metadata": {},
   "source": [
    "**Question:** By using the above functions, fill the code of the `policy_iteration_v(mdp)` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da751c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Policy Iteration with the V function -----------------#\n",
    "# Given an MDP, this algorithm simultaneously computes\n",
    "# the optimal state value function V and the optimal policy\n",
    "\n",
    "\n",
    "def policy_iteration_v(\n",
    "    mdp: MazeMDPEnv, render: bool = True\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    # policy iteration over the v function\n",
    "    v = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    v_list = []\n",
    "    policy = random_policy(mdp)\n",
    "\n",
    "    stop = False\n",
    "\n",
    "    mdp.init_draw(\"Policy Iteration (V)\")\n",
    "\n",
    "    while not stop:\n",
    "        vold = v.copy()\n",
    "\n",
    "        mdp.draw_v(v, title=\"Policy iteration Q\")\n",
    "\n",
    "        # Step 1 : Policy Evaluation\n",
    "        # To be completed...\n",
    "\n",
    "        v = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Step 2 : Policy Improvement\n",
    "        # To be completed...\n",
    "\n",
    "        policy = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Check convergence\n",
    "        if (np.linalg.norm(v - vold)) < 0.01:\n",
    "            stop = True\n",
    "        v_list.append(np.linalg.norm(v))\n",
    "\n",
    "    mdp.draw_v_pi(v, get_policy_from_v(mdp, v))\n",
    "\n",
    "    return v, v_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ad8a2",
   "metadata": {},
   "source": [
    "And finally run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b3d24",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "v, v_list = policy_iteration_v(env, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52b4ad",
   "metadata": {},
   "source": [
    "### Policy iteration with the $Q$ function ###\n",
    "\n",
    "The **policy iteration** algorithm with the $Q$ function is the same as with\n",
    "the $V$ function, but the policy improvement step is more straightforward.\n",
    "\n",
    "When using $Q$, the Bellman Optimality Equation with deterministic policy\n",
    "$\\pi$ for $Q$ becomes: $$Q^{\\pi}(s,a) = r(s,a) + \\gamma \\sum_{y \\in\n",
    "S}P(s,a,y)Q^{\\pi}(y,\\pi(y)).$$\n",
    "\n",
    "The policy can then be updated as follows: $$\\pi^{(t+1)}(s) =\n",
    "\\arg\\max_aQ^{\\pi^{(t)}}(s,a).$$\n",
    "\n",
    "First, we need to determine a policy from the $Q$ function.\n",
    "\n",
    "**Question:**  fill the `get_policy_from_q(q)` function, where $q$ is the\n",
    "state-action value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50df95f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_policy_from_q(q: np.ndarray) -> np.ndarray:\n",
    "    # Outputs a policy given the action values\n",
    "\n",
    "    return ...\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4ac88",
   "metadata": {},
   "source": [
    "**Question:** By drawing inspiration on the functions give with the $v$\n",
    "function, fill the code of the `evaluate_one_step_q(mdp, q, policy)` function\n",
    "below, where $q$ is some action value function, and the `evaluate_q(mdp,\n",
    "policy)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d1213",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def evaluate_one_step_q(\n",
    "    mdp: MazeMDPEnv, q: np.ndarray, policy: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    # Outputs the state value function after one step of policy evaluation\n",
    "    qnew = np.zeros(\n",
    "        (mdp.nb_states, mdp.action_space.n)\n",
    "    )  # initial action values are set to 0\n",
    "    for x in range(mdp.nb_states):  # for each state x\n",
    "        # Compute the value of the state x for each action u of the MDP action space\n",
    "        for u in range(mdp.action_space.n):\n",
    "            if x in mdp.terminal_states:\n",
    "                qnew[x, u] = mdp.r[x, u]\n",
    "            else:\n",
    "                # Process sum of the values of the neighbouring states\n",
    "                summ = 0\n",
    "                for y in range(mdp.nb_states):\n",
    "                    # To be completed...\n",
    "\n",
    "                    summ += ...\n",
    "                    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "                # To be completed...\n",
    "\n",
    "                qnew[x, u] = ...\n",
    "                assert False, 'Not implemented yet'\n",
    "\n",
    "    return qnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a29c8b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def evaluate_q(mdp: MazeMDPEnv, policy: np.ndarray) -> np.ndarray:\n",
    "    # Outputs the state value function of a policy\n",
    "    q = np.zeros(\n",
    "        (mdp.nb_states, mdp.action_space.n)\n",
    "    )  # initial action values are set to 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        qold = q.copy()\n",
    "\n",
    "        # To be completed...\n",
    "\n",
    "        q = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Test if convergence has been reached\n",
    "        if (np.linalg.norm(q - qold)) < 0.01:\n",
    "            stop = True\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2c00b",
   "metadata": {},
   "source": [
    "**Question:** By using the above functions, fill the code of the `policy_iteration_q(mdp)` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Policy Iteration with the Q function -----------------#\n",
    "# Given a MDP, this algorithm simultaneously computes\n",
    "# the optimal action value function Q and the optimal policy\n",
    "\n",
    "\n",
    "def policy_iteration_q(\n",
    "    mdp: MazeMDPEnv, render: bool = True\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"policy iteration over the q function.\"\"\"\n",
    "    q = np.zeros(\n",
    "        (mdp.nb_states, mdp.action_space.n)\n",
    "    )  # initial action values are set to 0\n",
    "    q_list = []\n",
    "    policy = random_policy(mdp)\n",
    "\n",
    "    stop = False\n",
    "\n",
    "    mdp.init_draw(\"Policy iteration Q\")\n",
    "\n",
    "    while not stop:\n",
    "        qold = q.copy()\n",
    "        mdp.draw_v(q)\n",
    "\n",
    "        # Step 1 : Policy evaluation\n",
    "\n",
    "        q = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Step 2 : Policy improvement\n",
    "\n",
    "        policy = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Check convergence\n",
    "        if (np.linalg.norm(q - qold)) <= 0.01:\n",
    "            stop = True\n",
    "        q_list.append(np.linalg.norm(q))\n",
    "\n",
    "    mdp.draw_v_pi(q, get_policy_from_q(q))\n",
    "    return q, q_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b231698a",
   "metadata": {},
   "source": [
    "Finally, run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19afa90b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "q, q_list = policy_iteration_q(env, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60427fbe",
   "metadata": {},
   "source": [
    "### Experimental comparisons\n",
    "\n",
    "We now compare the efficiency of the various dynamic programming methods using either the $V$ or the  $Q$ functions.\n",
    "\n",
    "In all your dymanic programming functions, add code to count the number of iterations and the number of elementary $V$ or $Q$ updates. Use the provided `mazemdp.Chrono` class to measure the time taken. You may generate various mazes of various sizes to figure out the influence of the maze topology.\n",
    "\n",
    "Build a table where you compare the various dymanic programming functions in terms of iterations, elementary operations and time taken.\n",
    "\n",
    "You can run the `plot_convergence_vi_pi(...)` function provided below to visualize the convergence of the various algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a1eaf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---- plot learning curves of Q-Learning and Sarsa using epsilon-greedy and softmax -----#\n",
    "\n",
    "\n",
    "def plot_convergence_vi_pi(m, render):\n",
    "    v, v_list1 = value_iteration_v(m, render)\n",
    "    q, q_list1 = value_iteration_q(m, render)\n",
    "    v, v_list2 = policy_iteration_v(m, render)\n",
    "    q, q_list2 = policy_iteration_q(m, render)\n",
    "\n",
    "    plt.plot(range(len(v_list1)), v_list1, label=\"value_iteration_v\")\n",
    "    plt.plot(range(len(q_list1)), q_list1, label=\"value_iteration_q\")\n",
    "    plt.plot(range(len(v_list2)), v_list2, label=\"policy_iteration_v\")\n",
    "    plt.plot(range(len(q_list2)), q_list2, label=\"policy_iteration_q\")\n",
    "\n",
    "    plt.xlabel(\"Number of episodes\")\n",
    "    plt.ylabel(\"Norm of V or Q value\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    # plt.savefig(\"comparison_DP.png\")\n",
    "    plt.title(\"Comparison of convergence rates\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc2701",
   "metadata": {},
   "source": [
    "**Question:** Run the code below and visualize the results of the different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90837e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence_vi_pi(env, False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
