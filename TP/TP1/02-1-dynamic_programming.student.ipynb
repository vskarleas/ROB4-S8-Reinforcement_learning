{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f794410c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    " Copyright Â© Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f2b4eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, we study the **value iteration** and **policy\n",
    "iteration** algorithms in a maze environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d9ebe",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4cad17a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'easypip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     IPython\u001b[38;5;241m.\u001b[39mApplication\u001b[38;5;241m.\u001b[39minstance()\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mdo_shutdown(\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01measypip\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m easyimport, easyinstall, is_notebook\n\u001b[1;32m     11\u001b[0m easyinstall(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbrl>=0.2.2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# easyinstall(\"bbrl_gymnasium>=0.3.5\")\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'easypip'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError:\n",
    "    !pip install --user easypip\n",
    "    # Automatically restart the kernel after installation\n",
    "    import IPython\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) \n",
    "\n",
    "from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "# easyinstall(\"bbrl_gymnasium>=0.3.5\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from moviepy.editor import ipython_display as video_display\n",
    "\n",
    "if not is_notebook():\n",
    "    print(\"Not displaying video (hidden since not in a notebook)\", file=sys.stderr)\n",
    "    def video_display(*args, **kwargs):\n",
    "        pass\n",
    "    def display(*args, **kwargs):\n",
    "        print(*args, **kwargs) \n",
    "\n",
    "testing_mode = os.environ.get(\"TESTING_MODE\", None) == \"ON\"\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "! pip install swig\n",
    "! pip install box2d-py\n",
    "! pip install bbrl_utils\n",
    "easyimport(\"bbrl_utils\").setup(maze_mdp=True)\n",
    "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
    "from mazemdp.mdp import Mdp\n",
    "from mazemdp.toolbox import egreedy, egreedy_loc, sample_categorical, softmax\n",
    "from mazemdp import random_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4355d1",
   "metadata": {},
   "source": [
    "# Agents and MDPs\n",
    "\n",
    "A reinforcement learning agent interacts with an environment represented as a\n",
    "Markov Decision Process (MDP). It is defined by a tuple $(S, A, P, r, \\gamma)$\n",
    "where $S$ is the state space, $A$ is the action space, $P(state_t, action_t,\n",
    "state_{t+1})$ is the transition function, $r(state_t, action_t)$ is the reward\n",
    "function and $\\gamma \\in [0, 1]$ is the discount factor.\n",
    "\n",
    "In what follows we import code to create an MDP corresponding to a random maze\n",
    "(see https://github.com/osigaud/SimpleMazeMDP for documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"MazeMDP-v0\",\n",
    "    kwargs={\"width\": 5, \"height\": 5, \"ratio\": 0.2},\n",
    "    render_mode=\"human\",\n",
    ")\n",
    "env = env.unwrapped\n",
    "env.reset()\n",
    "\n",
    "# in dynamic programming, there is no agent moving in the environment\n",
    "env.set_no_agent()\n",
    "env.init_draw(\"The maze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ccda8",
   "metadata": {},
   "source": [
    "Dynamic programming\n",
    "\n",
    "The goal of an RL agent is to find the optimal behaviour, defined by a policy\n",
    "$\\pi$ that assigns an action (or distribution over actions) to each state so\n",
    "as to maximize the agent's total expected reward. In order to estimate how\n",
    "good a state is, either a state value function $V(x)$ or a state-action value\n",
    "function $Q(x,u)$ is used.\n",
    "\n",
    "Dynamic programming algorithms are used for planning, they require a full\n",
    "knowledge of the MDP from the agent (in contrast to \"true\" RL where the agent\n",
    "does not know the transition and reward functions). They find the optimal\n",
    "policy by computing a value function $V$ or an action-value function $Q$ over\n",
    "the state space or state-action space of the given MDP. **Value iteration**\n",
    "and **policy iteration** are two standard dynamic programming algorithms. You\n",
    "should study both of them using both $V$ and $Q$, as these algorithms contain\n",
    "the basic building blocks for most RL algorithms.\n",
    "\n",
    "# Value Iteration\n",
    "\n",
    "## Value Iteration with the V function\n",
    "\n",
    "When using the $V$ function, **value iteration** aims at finding the optimal\n",
    "values $V^*$ based on the Bellman Optimality Equation: $$V^*(s) = \\max_a\n",
    "\\big[r(s,a) + \\gamma \\sum_{y \\in S} P(s,a,y)V^*(y) \\big],$$ where:\n",
    "\n",
    "*   $r(s, a)$ is the reward obtained from taking action $a$ in state $s$,\n",
    "*   $P(s, a, y)$ is the probability of reaching state $y$ when taking action\n",
    "    $a$ in state $s$,\n",
    "*   $\\gamma \\in [0,1]$ is a discount factor defining the relative importance\n",
    "    of long term rewards over short term ones (the closer to 0, the more the\n",
    "    agent focuses on immediate rewards).\n",
    "\n",
    "In practice, we start with an initial value function $V^0$ (for instance, the\n",
    "values of all states are 0), and then we iterate for all states $s$\n",
    "$$V^{i+1}(s) = \\max_a \\big[ r(s,a) + \\gamma \\sum_{y \\in S} P(s,a,y)V^i(y)\n",
    "\\big],$$\n",
    "\n",
    "until the values converge, that is $\\forall s, V^{i+1}(s) \\approx V^i(s)$. It\n",
    "is shown that at convergence, $\\forall s, V^i(s)= V^*(s)$.\n",
    "\n",
    "To visualize the policy obtained from **value iteration**, we need to first\n",
    "define the `get_policy_from_V()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1912057d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_policy_from_v(mdp: MazeMDPEnv, v: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Outputs a policy given the state values\"\"\"\n",
    "\n",
    "    # Sets initial state values are set to 0\n",
    "    policy = np.zeros(mdp.nb_states)  \n",
    "\n",
    "    # Loop over MDP states\n",
    "    for x in range(mdp.nb_states):\n",
    "        if x in mdp.terminal_states:\n",
    "            # Takes the reward associated with the terminal state\n",
    "            policy[x] = np.argmax(mdp.r[x, :])\n",
    "        else:\n",
    "            # Compute the value V(x) for state x\n",
    "            v_temp = []\n",
    "            \n",
    "            # Loop over actions\n",
    "            for u in range(mdp.action_space.n):\n",
    "                # Process sum of the values of the neighbouring states\n",
    "                summ = 0\n",
    "                for y in range(mdp.nb_states):\n",
    "                    summ = summ + mdp.P[x, u, y] * v[y]\n",
    "                v_temp.append(mdp.r[x, u] + mdp.gamma * summ)\n",
    "            policy[x] = np.argmax(v_temp)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495ef76",
   "metadata": {},
   "source": [
    "The `value_iteration_v(mdp)` function below provides the code of **value\n",
    "iteration** using the $V$ function. It is given as an example from which you\n",
    "can derive other instances of dynamic programming algorithms. Look at it more\n",
    "closely, this will help for later questions:\n",
    "\n",
    "* you can ignore the `mdp.new_render()` and `mdp.render(...)` functions which\n",
    "  are here to provide the visualization of the iterations.\n",
    "* find in the code the loop over states, the main loop that performs these\n",
    "  updates until the values don't change significantly anymore, the main update\n",
    "  equation. Found them? OK, you can continue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294822ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_v(\n",
    "    mdp: MazeMDPEnv, render: bool = True\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    # Value Iteration using the state value v\n",
    "    v = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    v_list = []\n",
    "    stop = False\n",
    "\n",
    "    mdp.init_draw(\"Value iteration V\")\n",
    "\n",
    "    mdp.draw_v(v)\n",
    "\n",
    "    while not stop:\n",
    "        v_old = v.copy()\n",
    "        mdp.draw_v(v)\n",
    "\n",
    "        for x in range(mdp.nb_states):  # for each state x\n",
    "            # Compute the value of the state x for each action u of the MDP action space\n",
    "            if x in mdp.terminal_states:\n",
    "                v[x] = np.max(mdp.r[x, :])\n",
    "            else:\n",
    "                v_temp = []\n",
    "                for u in range(mdp.action_space.n):\n",
    "                    # Process sum of the values of the neighbouring states\n",
    "                    summ = 0\n",
    "                    for y in range(mdp.nb_states):\n",
    "                        summ = summ + mdp.P[x, u, y] * v_old[y]\n",
    "                    v_temp.append(mdp.r[x, u] + mdp.gamma * summ)\n",
    "\n",
    "                # Select the highest state value among those computed\n",
    "                v[x] = np.max(v_temp)\n",
    "\n",
    "        # Test if convergence has been reached\n",
    "        if (np.linalg.norm(v - v_old)) < 0.01:\n",
    "            stop = True\n",
    "        v_list.append(np.linalg.norm(v))\n",
    "\n",
    "    policy = get_policy_from_v(mdp, v)\n",
    "    mdp.draw_v_pi(v, policy)\n",
    "    return v, v_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ff447",
   "metadata": {},
   "source": [
    "Let us run it on the previously defined MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91d537",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "v, v_list = value_iteration_v(env, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d50b096",
   "metadata": {},
   "source": [
    "### Value iteration with the $Q$ function ###\n",
    "\n",
    "The state-action value function $Q^{\\pi}(s,a)$ defines the value of being in\n",
    "state $s$, taking action $a$ then following policy $\\pi$. The Bellman\n",
    "Optimality Equation for $Q^*$ is $$ Q^*(s,a) =  r(s,a) + \\gamma \\sum_{y}\n",
    "P(s,a,y) \\max_{a'}Q^*(y,a'). $$\n",
    "\n",
    "**Question:** By taking inspiration from the `value_iteration_v(mdp)` function\n",
    "above, fill the blank (given with '\\#Q[x,u]=...') in the code of\n",
    "`value_iteration_q(mdp)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Value Iteration with the Q function ---------------------#\n",
    "# Given a MDP, this algorithm computes the optimal action value function Q\n",
    "# It then derives the optimal policy based on this function\n",
    "\n",
    "\n",
    "def value_iteration_q(\n",
    "    mdp: MazeMDPEnv, render: bool = True\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    q = np.zeros(\n",
    "        (mdp.nb_states, mdp.action_space.n)\n",
    "    )  # initial action values are set to 0\n",
    "    q_list = []\n",
    "    stop = False\n",
    "\n",
    "    mdp.init_draw(\"Value iteration Q\")\n",
    "\n",
    "    mdp.draw_v(q)\n",
    "\n",
    "    while not stop:\n",
    "        qold = q.copy()\n",
    "\n",
    "        mdp.draw_v(q)\n",
    "\n",
    "        for x in range(mdp.nb_states):\n",
    "            for u in range(mdp.action_space.n):\n",
    "                if x in mdp.terminal_states:\n",
    "                    q[x, u] = mdp.r[x, u]\n",
    "                else:\n",
    "                    summ = 0\n",
    "                    for y in range(mdp.nb_states):\n",
    "                        summ += mdp.P[x, u, y] * np.max(qold[y, :])\n",
    "\n",
    "                    # ComplÃ©ter d'aprÃ¨s la formule ci-dessus\n",
    "\n",
    "                    q[x, u] = ...\n",
    "                    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        if (np.linalg.norm(q - qold)) <= 0.01:\n",
    "            stop = True\n",
    "        q_list.append(np.linalg.norm(q))\n",
    "\n",
    "    mdp.draw_v(q)\n",
    "\n",
    "    return q, q_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91896bde",
   "metadata": {},
   "source": [
    "Once you are done, run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce41dc5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "q, q_list = value_iteration_q(env, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac3a4a4",
   "metadata": {},
   "source": [
    "## Policy Iteration ##\n",
    "\n",
    "The **policy iteration** algorithm is more complicated than **value\n",
    "iteration**. Given a MDP and a policy $\\pi$, **policy iteration** iterates the\n",
    "following steps:\n",
    "\n",
    "*   Evaluate policy $\\pi$: compute $V$ or $Q$ based on the policy $\\pi$;\n",
    "*   Improve policy $\\pi$: compute a better policy based on $V$ or $Q$.\n",
    "\n",
    "This process is repeated until convergence, i.e. when the policy cannot be\n",
    "improved anymore.\n",
    "\n",
    "### Policy iteration with the $V$ function ###\n",
    "\n",
    "When using $V$, $V^{\\pi}(s)$ is the expected return when starting from state\n",
    "$s$ and following policy $\\pi$. It is processed based on the Bellman\n",
    "Optimality Equation for deterministic policies:\n",
    "\n",
    "$$V^\\pi(s) = r(s, \\pi(s)) + \\gamma \\sum_{y \\in S}P(s, \\pi(s), y)V^\\pi(y),$$\n",
    "\n",
    "where:\n",
    "\n",
    "*   $\\pi$ is a deterministic policy, meaning that in a state $s$, the agent\n",
    "    always selects the same action,\n",
    "*   $V^\\pi(y)$ is the value of the state $y$ under policy $\\pi$.\n",
    "\n",
    "Thus, given a policy $\\pi$, one must first compute its value function\n",
    "$V^\\pi(s)$ for all states $s$ iterating the Bellman Optimality Equation until\n",
    "convergence, that is using **value iteration**. Then, one must determine if\n",
    "policy $\\pi$ can be improved based on $V$. For that, in each state $s$, one\n",
    "can compute the Q-value $Q(s,a)$ of applying action $a$ and then following\n",
    "policy $\\pi$ based on the just computed $V^\\pi$, and replace the action\n",
    "$\\pi(s)$ with $\\arg\\max_a Q(s,a)$.\n",
    "\n",
    "In order to facilitate the coding of **policy iteration** algorithms, we first\n",
    "define a set of useful functions.\n",
    "\n",
    "The `improve_policy_from_v(mdp, v, policy)` function is very similar to the\n",
    "`get_policy_from_v(v)` function which was given above. The main difference is\n",
    "that it takes a policy as argument and improves this policy when possible,\n",
    "thus is more in the spirit of the `policy improvement` step of **policy\n",
    "iteration**. But both functions can be used interchangeably.\n",
    "\n",
    "The functions `evaluate_one_step_v(mdp, v, policy)`, where `mdp` is a given\n",
    "MDP, `v` is some value function in this MDP and `policy` is some policy and\n",
    "the function `evaluate_v(mdp, policy)` are also given. These functions are\n",
    "used to build the value function $V^\\pi$ corresponding to policy $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f19b1b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def improve_policy_from_v(\n",
    "    mdp: MazeMDPEnv, v: np.ndarray, policy: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    # Improves a policy given the state values\n",
    "    for x in range(mdp.nb_states):  # for each state x\n",
    "        # Compute the value of the state x for each action u of the MDP action space\n",
    "        if x in mdp.terminal_states:\n",
    "            policy[x] = np.argmax(mdp.r[x, :])\n",
    "        else:\n",
    "            v_temp = np.zeros(mdp.action_space.n)\n",
    "            for u in range(mdp.action_space.n):\n",
    "                # Process sum of the values of the neighbouring states\n",
    "                summ = 0\n",
    "                for y in range(mdp.nb_states):\n",
    "                    summ = summ + mdp.P[x, u, y] * v[y]\n",
    "                v_temp[u] = mdp.r[x, u] + mdp.gamma * summ\n",
    "\n",
    "            for u in range(mdp.action_space.n):\n",
    "                if v_temp[u] > v_temp[policy[x]]:\n",
    "                    policy[x] = u\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40082d72",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def evaluate_one_step_v(\n",
    "    mdp: MazeMDPEnv, v: np.ndarray, policy: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    # Outputs the state value function after one step of policy evaluation\n",
    "    # Corresponds to one application of the Bellman Operator\n",
    "    v_new = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    for x in range(mdp.nb_states):  # for each state x\n",
    "        # Compute the value of the state x for each action u of the MDP action space\n",
    "        if x in mdp.terminal_states:\n",
    "            v_new[x] = mdp.r[x, policy[x]]\n",
    "        else:\n",
    "            # Process sum of the values of the neighbouring states\n",
    "            summ = 0\n",
    "            for y in range(mdp.nb_states):\n",
    "                summ = summ + mdp.P[x, policy[x], y] * v[y]\n",
    "            v_new[x] = mdp.r[x, policy[x]] + mdp.gamma * summ\n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc532bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def evaluate_v(mdp: MazeMDPEnv, policy: np.ndarray) -> np.ndarray:\n",
    "    # Outputs the state value function of a policy\n",
    "    v = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        vold = v.copy()\n",
    "        v = evaluate_one_step_v(mdp, vold, policy)\n",
    "\n",
    "        # Test if convergence has been reached\n",
    "        if (np.linalg.norm(v - vold)) < 0.01:\n",
    "            stop = True\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbfd5a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "To perform **policy iteration** we also need an initial random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mazemdp import random_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe994084",
   "metadata": {},
   "source": [
    "**Question:** By using the above functions, fill the code of the `policy_iteration_v(mdp)` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da751c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Policy Iteration with the V function -----------------#\n",
    "# Given an MDP, this algorithm simultaneously computes\n",
    "# the optimal state value function V and the optimal policy\n",
    "\n",
    "\n",
    "def policy_iteration_v(\n",
    "    mdp: MazeMDPEnv, render: bool = True\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    # policy iteration over the v function\n",
    "    v = np.zeros(mdp.nb_states)  # initial state values are set to 0\n",
    "    v_list = []\n",
    "    policy = random_policy(mdp)\n",
    "\n",
    "    stop = False\n",
    "\n",
    "    mdp.init_draw(\"Policy Iteration (V)\")\n",
    "\n",
    "    while not stop:\n",
    "        vold = v.copy()\n",
    "\n",
    "        mdp.draw_v(v, title=\"Policy iteration Q\")\n",
    "\n",
    "        # Step 1 : Policy Evaluation\n",
    "        # To be completed...\n",
    "\n",
    "        v = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Step 2 : Policy Improvement\n",
    "        # To be completed...\n",
    "\n",
    "        policy = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Check convergence\n",
    "        if (np.linalg.norm(v - vold)) < 0.01:\n",
    "            stop = True\n",
    "        v_list.append(np.linalg.norm(v))\n",
    "\n",
    "    mdp.draw_v_pi(v, get_policy_from_v(mdp, v))\n",
    "\n",
    "    return v, v_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ad8a2",
   "metadata": {},
   "source": [
    "And finally run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b3d24",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "v, v_list = policy_iteration_v(env, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52b4ad",
   "metadata": {},
   "source": [
    "### Policy iteration with the $Q$ function ###\n",
    "\n",
    "The **policy iteration** algorithm with the $Q$ function is the same as with\n",
    "the $V$ function, but the policy improvement step is more straightforward.\n",
    "\n",
    "When using $Q$, the Bellman Optimality Equation with deterministic policy\n",
    "$\\pi$ for $Q$ becomes: $$Q^{\\pi}(s,a) = r(s,a) + \\gamma \\sum_{y \\in\n",
    "S}P(s,a,y)Q^{\\pi}(y,\\pi(y)).$$\n",
    "\n",
    "The policy can then be updated as follows: $$\\pi^{(t+1)}(s) =\n",
    "\\arg\\max_aQ^{\\pi^{(t)}}(s,a).$$\n",
    "\n",
    "First, we need to determine a policy from the $Q$ function.\n",
    "\n",
    "**Question:**  fill the `get_policy_from_q(q)` function, where $q$ is the\n",
    "state-action value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50df95f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_policy_from_q(q: np.ndarray) -> np.ndarray:\n",
    "    # Outputs a policy given the action values\n",
    "\n",
    "    return ...\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4ac88",
   "metadata": {},
   "source": [
    "**Question:** By drawing inspiration on the functions give with the $v$\n",
    "function, fill the code of the `evaluate_one_step_q(mdp, q, policy)` function\n",
    "below, where $q$ is some action value function, and the `evaluate_q(mdp,\n",
    "policy)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d1213",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def evaluate_one_step_q(\n",
    "    mdp: MazeMDPEnv, q: np.ndarray, policy: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    # Outputs the state value function after one step of policy evaluation\n",
    "    qnew = np.zeros(\n",
    "        (mdp.nb_states, mdp.action_space.n)\n",
    "    )  # initial action values are set to 0\n",
    "    for x in range(mdp.nb_states):  # for each state x\n",
    "        # Compute the value of the state x for each action u of the MDP action space\n",
    "        for u in range(mdp.action_space.n):\n",
    "            if x in mdp.terminal_states:\n",
    "                qnew[x, u] = mdp.r[x, u]\n",
    "            else:\n",
    "                # Process sum of the values of the neighbouring states\n",
    "                summ = 0\n",
    "                for y in range(mdp.nb_states):\n",
    "                    # To be completed...\n",
    "\n",
    "                    summ += ...\n",
    "                    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "                # To be completed...\n",
    "\n",
    "                qnew[x, u] = ...\n",
    "                assert False, 'Not implemented yet'\n",
    "\n",
    "    return qnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a29c8b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def evaluate_q(mdp: MazeMDPEnv, policy: np.ndarray) -> np.ndarray:\n",
    "    # Outputs the state value function of a policy\n",
    "    q = np.zeros(\n",
    "        (mdp.nb_states, mdp.action_space.n)\n",
    "    )  # initial action values are set to 0\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        qold = q.copy()\n",
    "\n",
    "        # To be completed...\n",
    "\n",
    "        q = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Test if convergence has been reached\n",
    "        if (np.linalg.norm(q - qold)) < 0.01:\n",
    "            stop = True\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2c00b",
   "metadata": {},
   "source": [
    "**Question:** By using the above functions, fill the code of the `policy_iteration_q(mdp)` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Policy Iteration with the Q function -----------------#\n",
    "# Given a MDP, this algorithm simultaneously computes\n",
    "# the optimal action value function Q and the optimal policy\n",
    "\n",
    "\n",
    "def policy_iteration_q(\n",
    "    mdp: MazeMDPEnv, render: bool = True\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"policy iteration over the q function.\"\"\"\n",
    "    q = np.zeros(\n",
    "        (mdp.nb_states, mdp.action_space.n)\n",
    "    )  # initial action values are set to 0\n",
    "    q_list = []\n",
    "    policy = random_policy(mdp)\n",
    "\n",
    "    stop = False\n",
    "\n",
    "    mdp.init_draw(\"Policy iteration Q\")\n",
    "\n",
    "    while not stop:\n",
    "        qold = q.copy()\n",
    "        mdp.draw_v(q)\n",
    "\n",
    "        # Step 1 : Policy evaluation\n",
    "\n",
    "        q = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Step 2 : Policy improvement\n",
    "\n",
    "        policy = ...\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Check convergence\n",
    "        if (np.linalg.norm(q - qold)) <= 0.01:\n",
    "            stop = True\n",
    "        q_list.append(np.linalg.norm(q))\n",
    "\n",
    "    mdp.draw_v_pi(q, get_policy_from_q(q))\n",
    "    return q, q_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b231698a",
   "metadata": {},
   "source": [
    "Finally, run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19afa90b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "q, q_list = policy_iteration_q(env, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60427fbe",
   "metadata": {},
   "source": [
    "### Experimental comparisons\n",
    "\n",
    "We now compare the efficiency of the various dynamic programming methods using either the $V$ or the  $Q$ functions.\n",
    "\n",
    "In all your dymanic programming functions, add code to count the number of iterations and the number of elementary $V$ or $Q$ updates. Use the provided `mazemdp.Chrono` class to measure the time taken. You may generate various mazes of various sizes to figure out the influence of the maze topology.\n",
    "\n",
    "Build a table where you compare the various dymanic programming functions in terms of iterations, elementary operations and time taken.\n",
    "\n",
    "You can run the `plot_convergence_vi_pi(...)` function provided below to visualize the convergence of the various algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a1eaf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---- plot learning curves of Q-Learning and Sarsa using epsilon-greedy and softmax -----#\n",
    "\n",
    "\n",
    "def plot_convergence_vi_pi(m, render):\n",
    "    v, v_list1 = value_iteration_v(m, render)\n",
    "    q, q_list1 = value_iteration_q(m, render)\n",
    "    v, v_list2 = policy_iteration_v(m, render)\n",
    "    q, q_list2 = policy_iteration_q(m, render)\n",
    "\n",
    "    plt.plot(range(len(v_list1)), v_list1, label=\"value_iteration_v\")\n",
    "    plt.plot(range(len(q_list1)), q_list1, label=\"value_iteration_q\")\n",
    "    plt.plot(range(len(v_list2)), v_list2, label=\"policy_iteration_v\")\n",
    "    plt.plot(range(len(q_list2)), q_list2, label=\"policy_iteration_q\")\n",
    "\n",
    "    plt.xlabel(\"Number of episodes\")\n",
    "    plt.ylabel(\"Norm of V or Q value\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    # plt.savefig(\"comparison_DP.png\")\n",
    "    plt.title(\"Comparison of convergence rates\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc2701",
   "metadata": {},
   "source": [
    "**Question:** Run the code below and visualize the results of the different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90837e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence_vi_pi(env, False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
